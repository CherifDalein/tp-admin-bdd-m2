export BROKER="localhost:9092"
export KSQLDB_URL="http://localhost:8088"


verification
curl -s "$KSQLDB_URL/info" | jq .
curl -s "$KSQLDB_URL/healthcheck"


1) 

docker exec -i kafka-1 \
  kafka-topics --bootstrap-server "$BROKER" \
  --create --topic temperatures --partitions 4 --replication-factor 1 --if-not-exists


la commande correcte est:

docker exec -i kafka-1 \
  kafka-topics --bootstrap-server "$BROKER" --describe --topic temperatures

taper la commande pour executer le script:

bash -lc 'python3 - <<'"'"'PY'"'"' | docker exec -i kafka-1 \
  kafka-console-producer --bootstrap-server '"$BROKER"' \
  --topic temperatures --property parse.key=true --property key.separator=:
import json,random,time,sys
villes=["Clermont-Ferrand","Lyon","Paris","Bordeaux","Nantes"]
for _ in range(200):
    v=random.choice(villes)
    rec={"ville":v,"t":round(random.uniform(5,35),1),"ts":int(time.time()*1000)}
    print(f"{v}:{json.dumps(rec)}"); sys.stdout.flush(); time.sleep(0.2)
PY'


puis aller sur ce lien http://localhost:9021/ pour verifier

Et tu vÃ©rifies :

le topic temperatures

la quantitÃ© de messages dans chaque partition

Tu devrais voir une rÃ©partition trÃ¨s inÃ©gale.
Pourquoi ? Parce que la clÃ© dÃ©termine la partition via Murmur2.

Certaines villes tombent toujours sur la mÃªme partition.

ou bien avec cette commande

docker exec -i kafka-1 \
  kafka-topics --bootstrap-server "$BROKER" --describe --topic temperatures


  APres le script est legerement modifiÃ©

  # Option - kafka-console-producer (clÃ© via parse.key)
bash -lc 'python3 - <<'"'"'PY'"'"' | docker exec -i kafka-1 \
  kafka-console-producer --bootstrap-server '"$BROKER"' \
  --topic temperatures --property parse.key=true --property key.separator=:
import json,random,time,sys
villes=["Clermont-Ferrand","Lyon","Paris","Bordeaux","Montpellier"]
for _ in range(200):
    v=random.choice(villes)
    rec={"ville":v,"t":round(random.uniform(5,35),1),"ts":int(time.time()*1000)}
    print(f"{v}:{json.dumps(rec)}"); sys.stdout.flush(); time.sleep(0.2)
PY'


Question : quâ€™observez-vous sur la rÃ©partition des messages ?

ï‘‰ Tu observes que les messages sont rÃ©partis diffÃ©remment entre les partitions.

Plus prÃ©cisÃ©ment :

Les villes communes (Lyon, Paris, etc.) restent dans les mÃªmes partitions quâ€™avant, car leur hachage ne change pas.

Les messages de la nouvelle ville Montpellier tombent dans une autre partition, dÃ©terminÃ©e par sa valeur de hachage.

La rÃ©partition totale change, car Nantes et Montpellier nâ€™ont pas le mÃªme hash â†’ donc pas la mÃªme partition.

ï“Œ La clÃ© modifiÃ©e change la distribution globale.


Lien avec la fonction de hachage Murmur2

Kafka utilise :

ðŸ‘‰ Murmur2(key) % nombre_de_partitions
pour choisir la partition.

Donc :

chaque clÃ© tombe toujours dans la mÃªme partition, tant que le nombre de partitions ne change pas.

si tu remplaces une clÃ© â†’ tu changes son hash â†’ donc sa partition.

câ€™est exactement pour Ã§a que Nantes â‰  Montpellier â†’ distribution diffÃ©rente.


Remarque: La rÃ©partition des messages dÃ©pend entiÃ¨rement de la clÃ©. Kafka utilise la fonction de hachage Murmur2 pour calculer la partition : partition = murmur2(key) % numPartitions.

Lorsque nous remplaÃ§ons "Nantes" par "Montpellier", les messages associÃ©s Ã  cette clÃ© sont envoyÃ©s dans une partition diffÃ©rente, car leur valeur de hachage est diffÃ©rente.

Les autres villes conservent la mÃªme partition qu'avant car leur clÃ© nâ€™a pas changÃ©.

2)

Pour se connecter Ã  ksqlDB, jâ€™ai utilisÃ© lâ€™interface Confluent Control Center disponible sur http://localhost:9021.
Dans le menu KSQLDB Cluster, jâ€™ai ouvert le moteur ksqlDB et exÃ©cutÃ© la commande SHOW TOPICS;.
Cela mâ€™a permis de visualiser les topics temperatures (4 partitions) et commandes.
Lâ€™outil permet de vÃ©rifier facilement la prÃ©sence des topics Kafka et leur configuration sans passer par le CLI.

voici ce que j'ai eu apres la commandes show topics;
{
  "@type": "kafka_topics",
  "statementText": "SHOW TOPICS;",
  "topics": [
    {
      "name": "commandes",
      "replicaInfo": [
        3
      ]
    },
    {
      "name": "temperatures",
      "replicaInfo": [
        1,
        1,
        1,
        1
      ]
    }
  ],
  "warnings": [

  ]
}





3) j'ai tapÃ© la commande juste apres avoir modifiÃ© un peu le code du producer 
while True:
    ville = random.choice(villes)
    message = {"ville": ville, "t": round(random.uniform(5,35),1), "ts": int(time.time()*1000)}
    producer.produce("temperatures", key=ville, value=json.dumps(message))
    producer.flush()
    time.sleep(0.2)  # vitesse adaptÃ©e



CREATE STREAM S_TEMPS_RAW1 (
  ville STRING,
  t DOUBLE,
  ts BIGINT
) WITH (
  KAFKA_TOPIC='temperatures',
  VALUE_FORMAT='JSON',
  TIMESTAMP='ts'
);

"pir verofoer la creation j'ai tapÃ© 
SHOW STREAMS;
et j'ai vu le s_temps_raw1

et pour visualiser j'ai tapÃ©

SELECT * FROM S_TEMPS_RAW1 EMIT CHANGES;

pour voir les enregistrements de la ville paris, j'ai tapÃ©

SELECT * FROM S_TEMPS_RAW1 WHERE ville='Paris' EMIT CHANGES;

pour tout reafficher, on peut utiliser la commande

SELECT * FROM S_TEMPS_RAW
EMIT CHANGES
LIMIT 1000;


j'ai crÃ©Ã© un stream partitionnÃ© par ville (S_TEMPS_BY_VILLE)

CREATE STREAM S_TEMPS_BY_VILLE
WITH (KAFKA_TOPIC='temperatures_by_ville', PARTITIONS=4)
AS
SELECT ville, t, ts
FROM S_TEMPS_RAW1
PARTITION BY ville
EMIT CHANGES;

pour verifier j'ai tapÃ© Ã§a et Ã§a a bien marche

SHOW STREAMS;
DESCRIBE S_TEMPS_BY_VILLE;

ET enfin Dans le Control Center â†’ onglet Persistent Queries.

Pourquoi S_TEMPS_BY_VILLE est persistante :

Elle lit un stream existant (S_TEMPS_RAW1)

Elle Ã©crit en permanence dans un autre topic (temperatures_by_ville)

Elle reste active et continue de traiter les messages en temps rÃ©el

ðŸ”¹ Les requÃªtes persistantes sont donc â€œactivesâ€ tant que ksqlDB tourne.






