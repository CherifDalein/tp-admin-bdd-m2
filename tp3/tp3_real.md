<p align="center"><b>Mamadou Cherif DIALLO - Master 2 Informatique</b></p>

# TP3 ADMINISTRATION BASE DE DONNEES

## Objectifs

L‚Äôobjectif de ce TP est de se familiariser avec le traitement de flux en temps r√©el √† l‚Äôaide de Kafka et ksqlDB. Il s‚Äôagit d‚Äôapprendre √† cr√©er, manipuler et interroger des streams et des tables mat√©rialis√©es afin de comprendre comment les donn√©es en continu peuvent √™tre transform√©es, agr√©g√©es et analys√©es.
√Ä travers diff√©rentes op√©rations (cr√©ation de streams, jointures, fen√™tres temporelles TUMBLING et HOPPING, calcul de m√©triques en temps r√©el), ce TP permet d‚Äôacqu√©rir une compr√©hension concr√®te des concepts fondamentaux du stream processing.
Les commandes et observations r√©alis√©es montrent comment ksqlDB r√©agit √† l‚Äôarriv√©e continue de messages, et comment les agr√©gations se recalculent dynamiquement selon les fen√™tres d√©finies et les derni√®res valeurs re√ßues.

````md
# TP Kafka + ksqlDB  
## Rapport complet et structur√©

---

# üåê **Initialisation des variables d‚Äôenvironnement**

```bash
export BROKER="localhost:9092"
export KSQLDB_URL="http://localhost:8088"
````

## V√©rification du serveur ksqlDB

```bash
curl -s "$KSQLDB_URL/info" | jq .
curl -s "$KSQLDB_URL/healthcheck"
```

---

# 1Ô∏è‚É£ **Cr√©ation du topic `temperatures`**

### Commande correcte

```bash
docker exec -i kafka-1 \
  kafka-topics --bootstrap-server "$BROKER" \
  --create --topic temperatures --partitions 4 --replication-factor 1 --if-not-exists
```

### V√©rification du topic

```bash
docker exec -i kafka-1 \
  kafka-topics --bootstrap-server "$BROKER" --describe --topic temperatures
```

---

# 1Ô∏è‚É£.2 **Production de messages dans le topic**

### Script producer (version 1)

```bash
bash -lc 'python3 - <<'"'"'PY'"'"' | docker exec -i kafka-1 \
  kafka-console-producer --bootstrap-server '"$BROKER"' \
  --topic temperatures --property parse.key=true --property key.separator=:
import json,random,time,sys
villes=["Clermont-Ferrand","Lyon","Paris","Bordeaux","Nantes"]
for _ in range(200):
    v=random.choice(villes)
    rec={"ville":v,"t":round(random.uniform(5,35),1),"ts":int(time.time()*1000)}
    print(f"{v}:{json.dumps(rec)}"); sys.stdout.flush(); time.sleep(0.2)
PY'
```

---

# 1Ô∏è‚É£.3 **V√©rification dans le Control Center**

Acc√©der √† :

üëâ **[http://localhost:9021/](http://localhost:9021/)**

V√©rifier :

* Topic `temperatures`
* Nombre de messages dans chaque partition

üìå **Observation : r√©partition tr√®s in√©gale.**

**Explication :**
Kafka utilise :

```
partition = murmur2(key) % nb_partitions
```

Donc une ville = toujours la m√™me partition.

---

# 1Ô∏è‚É£.4 **Script modifi√© (Nantes ‚Üí Montpellier)**

```bash
bash -lc 'python3 - <<'"'"'PY'"'"' | docker exec -i kafka-1 \
  kafka-console-producer --bootstrap-server '"$BROKER"' \
  --topic temperatures --property parse.key=true --property key.separator=:
import json,random,time,sys
villes=["Clermont-Ferrand","Lyon","Paris","Bordeaux","Montpellier"]
for _ in range(200):
    v=random.choice(villes)
    rec={"ville":v,"t":round(random.uniform(5,35),1),"ts":int(time.time()*1000)}
    print(f"{v}:{json.dumps(rec)}"); sys.stdout.flush(); time.sleep(0.2)
PY'
```

### ‚ùì **Observation**

* Les villes existantes restent dans les m√™mes partitions (m√™me hash)
* Montpellier a un hash diff√©rent ‚Üí tombe dans une autre partition

---

# 2Ô∏è‚É£ **Connexion √† ksqlDB et exploration des topics**

Dans l‚Äôinterface KSQLDB du Control Center :

Menu ‚Üí **ksqlDB Cluster**

Commande :

```sql
SHOW TOPICS;
```

### R√©sultat observ√© :

```json
{
  "@type": "kafka_topics",
  "topics": [
    { "name": "commandes", "replicaInfo": [3] },
    { "name": "temperatures", "replicaInfo": [1,1,1,1] }
  ]
}
```

---

# 3Ô∏è‚É£ **Cr√©ation du stream brut S_TEMPS_RAW1**

```sql
CREATE STREAM S_TEMPS_RAW1 (
  ville STRING,
  t DOUBLE,
  ts BIGINT
) WITH (
  KAFKA_TOPIC='temperatures',
  VALUE_FORMAT='JSON',
  TIMESTAMP='ts'
);
```

### V√©rification

```sql
SHOW STREAMS;
```

![Mon image ](tp3.png)

### Visualisation du flux

Toutes les donn√©es :

```sql
SELECT * FROM S_TEMPS_RAW1 EMIT CHANGES;
```

Seulement Paris :

```sql
SELECT * FROM S_TEMPS_RAW1 WHERE ville='Paris' EMIT CHANGES;
```

---

# 3Ô∏è‚É£.2 **Cr√©ation du stream partitionn√© par ville**

```sql
CREATE STREAM S_TEMPS_BY_VILLE
WITH (KAFKA_TOPIC='temperatures_by_ville', PARTITIONS=4)
AS
SELECT ville, t, ts
FROM S_TEMPS_RAW1
PARTITION BY ville
EMIT CHANGES;
```

### V√©rification

```sql
SHOW STREAMS;
DESCRIBE S_TEMPS_BY_VILLE;
```

üìå **Pourquoi c‚Äôest une requ√™te persistante ?**

* lit un stream en continu
* √©crit en continu vers un topic
* maintient un √©tat ‚Üí requ√™te ACTIVE

---

# 4Ô∏è‚É£ **Fen√™tres TUMBLING (5 minutes)**

### Cr√©ation de la table T_MAX_5M

```sql
CREATE TABLE T_MAX_5M AS
SELECT
  ville,
  WINDOWSTART AS w_start,
  WINDOWEND   AS w_end,
  MAX(t)      AS t_max
FROM S_TEMPS_BY_VILLE
WINDOW TUMBLING (SIZE 5 MINUTES, GRACE PERIOD 30 SECONDS)
GROUP BY ville
EMIT CHANGES;
```

### Ce que l‚Äôon voit dans ‚ÄúPersistent Queries‚Äù

Une entr√©e comme :

| Query ID        | Type       | Source           | Sink     | Status  |
| --------------- | ---------- | ---------------- | -------- | ------- |
| CSAS_T_MAX_5M_1 | PERSISTENT | S_TEMPS_BY_VILLE | T_MAX_5M | RUNNING |

### Visualisation

```sql
SELECT * FROM T_MAX_5M EMIT CHANGES;
```

üìå **Interpr√©tation :**

* chaque ligne = une fen√™tre de 5 min
* `t_max` = temp√©rature maximale de la fen√™tre
* nouvelle fen√™tre toutes les 5 min

---

# 5Ô∏è‚É£ **Table des derni√®res valeurs (T_LAST)**

```sql
CREATE TABLE T_LAST AS
SELECT ville,
       LATEST_BY_OFFSET(t)  AS t_last,
       LATEST_BY_OFFSET(ts) AS ts_last
FROM S_TEMPS_BY_VILLE
GROUP BY ville
EMIT CHANGES;
```

### Comment v√©rifier ?

Snapshot :

```sql
SELECT * FROM T_LAST;
```

En continu :

```sql
SELECT * FROM T_LAST EMIT CHANGES;
```

### Derni√®re temp√©rature pour Lyon :

```sql
SELECT t_last, ts_last
FROM T_LAST
WHERE ville = 'Lyon'
EMIT CHANGES;
```

---

# 7Ô∏è‚É£ **Fen√™tres HOPPING (10 min, avance 2 min)**

Cr√©ation :

```sql
CREATE TABLE T_AVG_10M_HOP2 AS
SELECT
  ville,
  WINDOWSTART AS w_start,
  WINDOWEND   AS w_end,
  AVG(t)      AS t_avg
FROM S_TEMPS_BY_VILLE
WINDOW HOPPING (SIZE 10 MINUTES, ADVANCE BY 2 MINUTES)
GROUP BY ville
EMIT CHANGES;
```

### Visualisation

```sql
SELECT * FROM T_AVG_10M_HOP2 EMIT CHANGES;
```

üìå **Explication claire**

* Fen√™tre de 10 minutes
* Une nouvelle fen√™tre commence toutes les 2 minutes
* Les fen√™tres se chevauchent
* Pour une m√™me ville ‚Üí plusieurs lignes simultan√©es

---
